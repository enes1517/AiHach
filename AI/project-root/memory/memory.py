# memory/memory.py - DÃ¼zeltilmiÅŸ versiyon
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

# ğŸ”‘ Gemini modelini baÅŸlat
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    google_api_key=os.getenv("GEMINI_API_KEY")
)

# ğŸ’¬ GLOBAL session store - Bu Ã§ok Ã¶nemli!
session_store = {}

def get_session_history(session_id: str) -> ChatMessageHistory:
    """Session ID'ye gÃ¶re chat history dÃ¶ndÃ¼r - eÄŸer yoksa oluÅŸtur"""
    if session_id not in session_store:
        session_store[session_id] = ChatMessageHistory()
    return session_store[session_id]

# ğŸ§  GeliÅŸtirilmiÅŸ PromptTemplate
explanation_prompt = PromptTemplate.from_template("""
Sen Trendyol Ã¼rÃ¼n Ã¶nerisi yapan bir AI asistanÄ±sÄ±n. 

KullanÄ±cÄ±nÄ±n mevcut sorusu: "{input}"

GeÃ§miÅŸ konuÅŸmalarÄ±nÄ± gÃ¶z Ã¶nÃ¼nde bulundurarak yanÄ±t ver:
- EÄŸer daha Ã¶nce Ã¼rÃ¼n Ã¶nerileri yaptÄ±ysan ve kullanÄ±cÄ± o Ã¶nerilerle ilgili soru soruyorsa, Ã¶nceki Ã¶nerilerini referans al
- KullanÄ±cÄ±nÄ±n tercihlerini hatÄ±rla
- TutarlÄ± ve samimi bir dille cevap ver
- Ã–nceki konuÅŸmalarla Ã§eliÅŸme

YanÄ±t formatÄ±:
- Hangi Ã¼rÃ¼nler neden Ã¶neriliyor?
- KullanÄ±cÄ±ya en uygun olanlar hangileri?
- KarÅŸÄ±laÅŸtÄ±rmalÄ± Ã¶nerilerin varsa yaz

Samimi ama profesyonel ol.
""")

def run_llm(input_data: dict):
    """LLM'i Ã§alÄ±ÅŸtÄ±r ve hafÄ±za ile birlikte yanÄ±t Ã¼ret"""
    user_input = input_data.get("input", "")
    
    # Prompt'u formatla
    formatted_prompt = explanation_prompt.format_prompt(input=user_input).to_string()
    
    # LLM'den yanÄ±t al
    response = llm.invoke(formatted_prompt)
    
    # Content'i Ã§Ä±kar
    if hasattr(response, "content"):
        content = response.content
    else:
        content = str(response)
    
    return {
        "memory_response": content,
        "explanation": content
    }

# ğŸ“¦ Conversation chain - session store ile
conversation_chain = RunnableWithMessageHistory(
    RunnableLambda(run_llm),
    get_session_history,  # Function reference, Ã§aÄŸÄ±rma deÄŸil!
    input_messages_key="input",
    history_messages_key="history"
)

# ğŸ”§ Session temizleme fonksiyonu (isteÄŸe baÄŸlÄ±)
def clear_session(session_id: str):
    """Belirli bir session'Ä± temizle"""
    if session_id in session_store:
        del session_store[session_id]

def get_conversation_history(session_id: str) -> list:
    """Session'Ä±n konuÅŸma geÃ§miÅŸini dÃ¶ndÃ¼r"""
    if session_id in session_store:
        return session_store[session_id].messages
    return []